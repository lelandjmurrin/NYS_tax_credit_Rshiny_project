---
title: "R Notebook"
output: html_notebook
---
Call Libraries
```{r} 
library(tidyverse)
library(caret)
library(MASS)
library(car)
library(moments)
```


Calling the Transformed Datasets
```{r}
income_cleaned = read_csv('NYS_Corp_Tax_Credit_data/income_cleaned.csv')
income_cleaned
industry_cleaned = read_csv('NYS_Corp_Tax_Credit_data/industry_cleaned.csv')
industry_cleaned
```


Creating the Models
```{r}
sat.model.summary <- function (df, field, sat.formula){
    
    #Shapiro-Wilks test to evaluate normality
    print(shapiro.test(df[[field]]))
    
    #Kurtosis evaluation (normal distribution has a value close to 3)
    print('kurtosis')
    print(kurtosis(df[[field]]))
    linear.model.cleaned = lm(sat.formula, data = df)
    print(summary(linear.model.cleaned))
    plot(linear.model.cleaned)
    
    #histograms of response variable to check distribution
    print(df %>% 
      ggplot(aes_string(field)) + 
      geom_histogram() + 
      labs(title = 'Average Credit Amount Distribution') + 
      theme(plot.title = element_text(hjust = 0.5)))
    
    #Checking multicollinearity using VIF measurement
    print(vif(linear.model.cleaned))
    influencePlot(linear.model.cleaned)
    #avPlots(linear.model.cleaned)
}


sat.formula <- Avg ~ . - Amount
sat.field <- 'Avg'

sat.model.summary(income_cleaned, sat.field, sat.formula)
income.model <- lm(sat.formula, data = income_cleaned)

sat.model.summary(industry_cleaned, sat.field, sat.formula)
industry.model <- lm(sat.formula, data = industry_cleaned)
```

Correcting violation of Normality in previous model with BoxCox transform
```{r}
bc_func <- function (lm.cleaned, lambda.range){
  bc = boxCox(lm.cleaned, lambda = lambda.range)
  #Extracting the best lambda value.
  return(bc$x[which(bc$y == max(bc$y))])
}
  
#Income Group Dataset
income.lambda.bc = bc_func(income.model, seq(-0.2, 0.2, 1/10))
income.lambda.bc

#Industry Group Dataset
industry.lambda.bc = bc_func(industry.model, seq(-0.2, 0.2, 1/10))
industry.lambda.bc

bc_transform <- function(df, lambda.bc){
  return (df %>% 
            mutate(Avg.bc = (Avg^lambda.bc -1)/lambda.bc) %>% 
            select(-c(Avg, Amount)))
}

#Income Group Dataset
income_cleaned_bc <- bc_transform(income_cleaned, income.lambda.bc)
income.model.bc = lm(Avg.bc ~ ., data = income_cleaned_bc)

#Industry Group Dataset
industry_cleaned_bc <- bc_transform(industry_cleaned, industry.lambda.bc)
industry.model.bc = lm(Avg.bc ~ ., data = industry_cleaned_bc)
```

Checking linear regression assumptions for the transformed data.
```{r}
sat.formula.bc <- Avg.bc ~ .
sat.field.bc <- 'Avg.bc'

#Income
sat.model.summary(income_cleaned_bc, sat.field.bc, sat.formula.bc)

#Industry
sat.model.summary(industry_cleaned_bc, sat.field.bc, sat.formula.bc)
```

BIC comparison before and after BoxCox transform
```{r}
BIC(income.model.bc, income.model)
BIC(industry.model.bc, industry.model)
```

Stepwise Regression on Income_cat_bc (boxcox transformed dataset)
```{r}
#creating dummy variable columns for stepwise
dummy_func <- function (df){
  x = model.matrix(Avg.bc ~., df)[, -1]
  dummy_bc = as.data.frame(x) %>% mutate(Avg.bc = df$Avg.bc)
  #colnames(dummy_bc) <- str_replace_all(colnames(dummy_bc), "-|'|/| |,|�|&" , '_')
  colnames(dummy_bc) <- str_replace_all(colnames(dummy_bc), "[-'/ ,�&()`]" , '_')
  return(dummy_bc)
}
```

Cleaning column names further so stepwise regression doesn't present any errors
```{r}
#Income Group Dataset
income.dummy.bc <- dummy_func(income_cleaned_bc)
#colnames(income.dummy.bc)[37] <- 'NameManufactureru0092s_Real_Property_Tax_Credit'
colnames(income.dummy.bc)

#Industry Group Dataset
industry.dummy.bc <- dummy_func(industry_cleaned_bc)
#colnames(industry.dummy.bc)[35] <- 'NameManufactureru0092s_Real_Property_Tax_Credit'
colnames(industry.dummy.bc)
```

Stepwise regression using BIC as the criteria (k = log(n)).
```{r}
#Creating Stepwise Models
bcs = list(income = income.dummy.bc, industry = industry.dummy.bc)
model.fulls = list(income = lm(Avg.bc ~ ., data = income.dummy.bc), industry = lm(Avg.bc ~ ., data = industry.dummy.bc)) #All variables
model.emptys = list(income = lm(Avg.bc ~ 1, data = income.dummy.bc), industry = lm(Avg.bc ~ 1, data = industry.dummy.bc)) #intercept only
k = c('income', 'industry')
forwardBIC = list(income = NULL, industry = NULL)
backwardBIC = list(income = NULL, industry = NULL)

for (i in k){
  bc = bcs[[i]]
  scope = list(lower = formula(model.emptys[[i]]), upper = formula(model.fulls[[i]]))
  n_obs = bc %>% count() %>% dplyr::first()
  forwardBIC[[i]] = step(model.emptys[[i]], scope, direction = "forward", k = log(n_obs))
  backwardBIC[[i]] = step(model.fulls[[i]], scope, direction = "backward", k = log(n_obs))
}
```

Selecting Best Formula per Dataset from Stepwise Regressions
```{r}
bic_func <- function (BIC.model){
  print('Adjusted R Squared:')
  print(summary(BIC.model)$adj.r.squared)
  print('Number of Coefficients:')
  print(dim(summary(BIC.model)$coefficient)[1])
  print('VIF Check: ')
  print(max(vif(BIC.model)))
  print("*************************")
}

bic_func(forwardBIC[['income']])
bic_func(backwardBIC[['income']])
bic_func(forwardBIC[['industry']]) 
bic_func(backwardBIC[['industry']])



#Manual reduction of variables using VIF and then checked versus saturated model with Anova. This was not used because the saturated model contained multicollinearity issues as indicated by a high VIF score on some coefficients. And the anova test suggested that the coefficients removed in the reduced model were informative in our model, so we couldn't use it either. Thus Stepwise reduction is the preferred method for best model fit.

# VIF.variables <- as.data.frame(vif(model.fulls[['industry']])) %>% 
#   select(VIF = `vif(model.fulls[["industry"]])`) %>% 
#   filter(VIF > 5) %>% rownames()
# 
# industry.dummy.bc.VIF <- industry.dummy.bc %>% select(-all_of(VIF.variables))
# industry.model.VIF <- lm(Avg.bc ~ ., data = industry.dummy.bc.VIF)
# summary(industry.model.VIF)
# anova(industry.model.VIF, model.fulls[['industry']])

```

Best Model Selection from Stepwise
```{r}
#The Best Models selected for both income and industry were forwardBIC.

#Income Group Dataset
income.best.formula <- forwardBIC[['income']]$call[[2]]
income.best.formula

#Industry Group Dataset
industry.best.formula <- forwardBIC[['industry']]$call[[2]]
industry.best.formula
```

Splitting data up into test data and training data (test data is for year 2019, training is the rest)
```{r}
test_train_split <- function(dummy_bc, best.formula) {
  # data.test <- dummy_bc %>% filter(Year == 2019)
  # data.train <- dummy_bc %>% filter(Year != 2019)
  set.seed(0)
  train.i = sample(1:nrow(dummy_bc), 0.8*nrow(dummy_bc), replace = F)
  data.train <- dummy_bc %>% slice(train.i)
  data.test <- dummy_bc %>% slice(-train.i)
  
  #train
  #X.train <- as.matrix(data.train %>% select(-Avg.bc))
  X.train <- model.matrix(best.formula, data = data.train)[,-1]
  y.train <- as.matrix(data.train %>% select(Avg.bc))

  #test
  #X.test <- as.matrix(data.test %>% select(-Avg.bc))
  X.test <- model.matrix(best.formula, data = data.test)[,-1]
  y.test <- as.matrix(data.test %>% select(Avg.bc))
  return (list('X.train' = X.train, 'y.train' = y.train, 'X.test' = X.test, 'y.test' = y.test, 'data.train' = data.train, 'data.test' = data.test))
}

#Income
income.data.split.sat <- test_train_split(income.dummy.bc, Avg.bc ~ .)
income.data.split <- test_train_split(income.dummy.bc, income.best.formula)

#Industry
industry.data.split.sat <- test_train_split(industry.dummy.bc, Avg.bc ~ .)
industry.data.split <- test_train_split(industry.dummy.bc, industry.best.formula)
```


Lasso regression for comparison to Forward Stepwise
```{r}
# calc_MSE <- function (model, x.test, y.test){
#   y.predict <- predict(model, newdata = x.test)
#   return(mean((y.predict - y.test)^2))
# }

xy.splits <- list('income' = income.data.split, 
                  'industry' = industry.data.split, 
                  'income.sat' = income.data.split.sat, 
                  'industry.sat' = industry.data.split.sat)

lasso.list <- c('income', 'industry', 'income.sat', 'industry.sat')
baseline.list <- c('income.sat', 'industry.sat', 'income.sat', 'industry.sat')
ridge.alpha <- 0

for (a in 1:4){
  i = lasso.list[a]
  b = baseline.list[a]
  X.train <- xy.splits[[i]][['X.train']]
  y.train <- xy.splits[[i]][['y.train']]
  X.test <- xy.splits[[i]][['X.test']]
  y.test <- xy.splits[[i]][['y.test']]
  X.test.sat <- xy.splits[[b]][['X.test']]
  y.test.sat <- xy.splits[[b]][['y.test']]
  data.train <- xy.splits[[i]][['data.train']]
  data.test <- xy.splits[[i]][['data.test']]
  
  #create lambda grid
  lambda.grid = 10^seq(2, -5, length = 100)
  
  #create lasso models with lambda.grid
  lasso.models = glmnet(X.train, y.train, alpha = ridge.alpha, lambda = lambda.grid)
  
  #visualize coefficient shrinkage
  plot(lasso.models, xvar = "lambda", label = TRUE, main = paste("Lasso Regression:", i))
  
  #Cross Validation to find best lambda
  set.seed(0)
  cv.lasso.models <- cv.glmnet(X.train, y.train, alpha = ridge.alpha, lambda = lambda.grid, nfolds = 10)
  
  #visualize cross validation for lambda that minimizes the mean squared error.
  plot(cv.lasso.models, main = paste("Lasso Regression:", i))
  
  #Checking the best lambda
  log(cv.lasso.models$lambda.min)
  best.lambda <- cv.lasso.models$lambda.min
  print(paste(i, ' best.lambda:', best.lambda))
  # best lambda with all the variables was found to be 0.0006892612
  # best lambda with only the bwdBIC coefficients included was found to be 0.0003053856
  
  #looking at the lasso coefficients for the best.lambda
  best.lambda.coeff <- predict(lasso.models, s = best.lambda, type = "coefficients")
  print('Number of Coefficients:')
  print(dim(best.lambda.coeff)[1])
  
  #fitting a model with the best lambda found to be 0.000689 and using it to make predictions for the testing data.
  lasso.best.lambda.train.pred <- predict(lasso.models, s = best.lambda, newx = X.test)
  lasso.best.lambda.train.pred
  
  #checking MSE
  MSE.lasso <- mean((lasso.best.lambda.train.pred - y.test)^2)
  sat.model.bc <- lm(Avg.bc ~., data = data.train)
  
  
  temp.df <- as.data.frame(X.test.sat) #temp fix
  colnames(temp.df) <- str_replace_all(colnames(temp.df), "[`]", '')
  
  y.predict <- predict(sat.model.bc, newdata = temp.df)
  MSE.sat <- mean((y.predict - y.test.sat)^2)
  
  print(paste(i, ' Lasso MSE: ', MSE.lasso, ' ', b, ' Saturated MSE: ', MSE.sat))

  metrics <- eval_results(y.test, lasso.best.lambda.train.pred, data.test)
  print(metrics)
  print('********************************')
}
```


```{r}
all.dfs <- list(income_cleaned, income_cleaned_bc, income.dummy.bc)
all.splits <- list(
  income_cleaned = test_train_split(income_cleaned, Avg ~ .),
  income_cleaned_bc = test_train_split(income_cleaned_bc, Avg.bc ~ .),
  income.data.split.sat = test_train_split(income.dummy.bc, Avg.bc ~ .),
  income.data.split = test_train_split(income.dummy.bc, income.best.formula)
)

for (i in all.splits) {
  print(i)
}

```


```{r}
# sat.model.bc <- lm(Avg.bc ~., data = xy.splits.sat[['industry']][['data.train']])
# 
# temp.df <- as.data.frame(xy.splits.sat[['industry']][['X.test']])
# 
# colnames(temp.df)[57]
# 
# colnames(temp.df) <- str_replace_all(colnames(temp.df), "[`]", '')
# colnames(temp.df)
# 
# y.predict <- predict(sat.model.bc, newdata = temp.df)
# MSE.sat <- mean((y.predict - xy.splits.sat[['industry']][['y.test']])^2)
# 
# print(paste(i, ' Lasso MSE: ', MSE.lasso, ' Saturated MSE: ', MSE.sat))
# 
# summary(sat.model.bc)$coefficients
# as.data.frame(xy.splits.sat[['industry']][['X.test']]) %>% select(starts_with('`GroupOth'))

```


```{r}
0.7293

# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  
  # Model performance metrics
  return(c(RMSE = RMSE,
  Rsquare = R_square))
  
}

# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
eval_results(y_train, predictions_train, train)

# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
eval_results(y_test, predictions_test, test)

summary(income.model.bc)
income.dummy.bc
```







```{r}
as.data.frame(lasso.best.lambda.train.pred) %>% mutate(Avg_in_dollars = (lasso.best.lambda.train.pred*lambda.bc+1)^(1/lambda.bc))
```

```{r}
income.sat.data.split <- test_train_split(income.dummy.bc, Avg.bc ~ .)
income.sat.data.split[['X.train']]

sat.model.bc <- lm(Avg.bc ~., data = as.data.frame(cbind(income.sat.data.split[['X.train']], income.sat.data.split[['y.train']])))
summary(sat.model.bc)

calc_MSE(sat.model.bc, as.data.frame(income.sat.data.split[['X.test']]), income.sat.data.split[['y.test']])


sat.y.predict <- predict(sat.model.bc, newdata = as.data.frame(income.sat.data.split[['X.test']]))
mean((sat.y.predict - income.sat.data.split[['y.test']])^2)
```

