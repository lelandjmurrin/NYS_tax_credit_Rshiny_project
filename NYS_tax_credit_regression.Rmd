---
title: "R Notebook"
output: html_notebook
---
Call Libraries
```{r} 
library(tidyverse)
library(caret)
library(MASS)
library(car)
library(moments)
```


Calling the Transformed Datasets
```{r}
income_cleaned = read_csv('NYS_Corp_Tax_Credit_data/income_cleaned.csv')
income_cleaned = income_cleaned %>% select(-Amount)

industry_cleaned = read_csv('NYS_Corp_Tax_Credit_data/industry_cleaned.csv')
industry_cleaned = industry_cleaned %>% select(-Amount)
```


Creating the Models
```{r}
sat.model.summary <- function (df, field, sat.formula){
    
    #Shapiro-Wilks test to evaluate normality
    print(shapiro.test(df[[field]]))
    
    #Kurtosis evaluation (normal distribution has a value close to 3)
    print('kurtosis')
    print(kurtosis(df[[field]]))
    linear.model.cleaned = lm(sat.formula, data = df)
    print(summary(linear.model.cleaned))
    plot(linear.model.cleaned)
    
    #histograms of response variable to check distribution
    print(df %>% 
      ggplot(aes_string(field)) + 
      geom_histogram() + 
      labs(title = 'Average Credit Amount Distribution') + 
      theme(plot.title = element_text(hjust = 0.5)))
    
    #Checking multicollinearity using VIF measurement
    print(vif(linear.model.cleaned))
    influencePlot(linear.model.cleaned)
    #avPlots(linear.model.cleaned)
}


sat.formula <- Avg ~ .
sat.field <- 'Avg'

sat.model.summary(income_cleaned, sat.field, sat.formula)
income.model <- lm(sat.formula, data = income_cleaned)

sat.model.summary(industry_cleaned, sat.field, sat.formula)
industry.model <- lm(sat.formula, data = industry_cleaned)
```


```{r}
plot(income.model, which = 6)
plot(income.model, which = 1)
```

Correcting violation of Normality in previous model with BoxCox transform
```{r}
bc_func <- function (lm.cleaned, lambda.range){
  bc = boxCox(lm.cleaned, lambda = lambda.range)
  #Extracting the best lambda value.
  return(bc$x[which(bc$y == max(bc$y))])
}

#Income Group Dataset
income.lambda.bc = bc_func(income.model, seq(-0.2, 0.2, 1/10))
income.lambda.bc

#Industry Group Dataset
industry.lambda.bc = bc_func(industry.model, seq(-0.2, 0.2, 1/10))
industry.lambda.bc

bc_transform <- function(df, lambda.bc){
  return (df %>% 
            mutate(Avg.bc = (Avg^lambda.bc -1)/lambda.bc) %>% 
            select(-c(Avg))) #took out field Amount
}

#Income Group Dataset
income_cleaned_bc <- bc_transform(income_cleaned, income.lambda.bc)
income.model.bc = lm(Avg.bc ~ ., data = income_cleaned_bc)

#Industry Group Dataset
industry_cleaned_bc <- bc_transform(industry_cleaned, industry.lambda.bc)
industry.model.bc = lm(Avg.bc ~ ., data = industry_cleaned_bc)
```

Checking linear regression assumptions for the transformed data.
```{r}
sat.formula.bc <- Avg.bc ~ .
sat.field.bc <- 'Avg.bc'

#Income
sat.model.summary(income_cleaned_bc, sat.field.bc, sat.formula.bc)

#Industry
sat.model.summary(industry_cleaned_bc, sat.field.bc, sat.formula.bc)
```

BIC comparison before and after BoxCox transform
```{r}
BIC(income.model.bc, income.model)
BIC(industry.model.bc, industry.model)
```

Stepwise Regression on Income_cat_bc (boxcox transformed dataset)
```{r}
#creating dummy variable columns for stepwise
dummy_func <- function (df){
  x = model.matrix(Avg.bc ~., df)[, -1]
  dummy_bc = as.data.frame(x) %>% mutate(Avg.bc = df$Avg.bc)
  #colnames(dummy_bc) <- str_replace_all(colnames(dummy_bc), "-|'|/| |,|�|&" , '_')
  colnames(dummy_bc) <- str_replace_all(colnames(dummy_bc), "[-'/ ,�&()`]" , '_')
  return(dummy_bc)
}
```

Cleaning column names further so stepwise regression doesn't present any errors
```{r}
#Income Group Dataset
income.dummy.bc <- dummy_func(income_cleaned_bc)
#colnames(income.dummy.bc)[37] <- 'NameManufactureru0092s_Real_Property_Tax_Credit'
colnames(income.dummy.bc)

#Industry Group Dataset
industry.dummy.bc <- dummy_func(industry_cleaned_bc)
#colnames(industry.dummy.bc)[35] <- 'NameManufactureru0092s_Real_Property_Tax_Credit'
colnames(industry.dummy.bc)
```

Stepwise regression using BIC as the criteria (k = log(n)).
```{r}
#Creating Stepwise Models
bcs = list(income = income.dummy.bc, industry = industry.dummy.bc)
model.fulls = list(income = lm(Avg.bc ~ ., data = income.dummy.bc), industry = lm(Avg.bc ~ ., data = industry.dummy.bc)) #All variables
model.emptys = list(income = lm(Avg.bc ~ 1, data = income.dummy.bc), industry = lm(Avg.bc ~ 1, data = industry.dummy.bc)) #intercept only
k = c('income', 'industry')
forwardBIC = list(income = NULL, industry = NULL)
backwardBIC = list(income = NULL, industry = NULL)

for (i in k){
  bc = bcs[[i]]
  scope = list(lower = formula(model.emptys[[i]]), upper = formula(model.fulls[[i]]))
  n_obs = bc %>% count() %>% dplyr::first()
  forwardBIC[[i]] = step(model.emptys[[i]], scope, direction = "forward", k = log(n_obs))
  backwardBIC[[i]] = step(model.fulls[[i]], scope, direction = "backward", k = log(n_obs))
}
```

Selecting Best Formula per Dataset from Stepwise Regressions
```{r}
bic_func <- function (BIC.model){
  print('Adjusted R Squared:')
  print(summary(BIC.model)$adj.r.squared)
  print('Number of Coefficients:')
  print(dim(summary(BIC.model)$coefficient)[1])
  print('VIF Check: ')
  print(max(vif(BIC.model)))
  print("*************************")
}

bic_func(forwardBIC[['income']])
bic_func(backwardBIC[['income']])
bic_func(forwardBIC[['industry']]) 
bic_func(backwardBIC[['industry']])



#Manual reduction of variables using VIF and then checked versus saturated model with Anova. This was not used because the saturated model contained multicollinearity issues as indicated by a high VIF score on some coefficients. And the anova test suggested that the coefficients removed in the reduced model were informative in our model, so we couldn't use it either. Thus Stepwise reduction is the preferred method for best model fit.

# VIF.variables <- as.data.frame(vif(model.fulls[['industry']])) %>% 
#   select(VIF = `vif(model.fulls[["industry"]])`) %>% 
#   filter(VIF > 5) %>% rownames()
# 
# industry.dummy.bc.VIF <- industry.dummy.bc %>% select(-all_of(VIF.variables))
# industry.model.VIF <- lm(Avg.bc ~ ., data = industry.dummy.bc.VIF)
# summary(industry.model.VIF)
# anova(industry.model.VIF, model.fulls[['industry']])

```

Best Model Selection from Stepwise
```{r}
#The Best Models selected for both income and industry were forwardBIC.

#Income Group Dataset
income.best.formula <- forwardBIC[['income']]$call[[2]]
income.best.formula

#Industry Group Dataset
industry.best.formula <- forwardBIC[['industry']]$call[[2]]
industry.best.formula
```


Splitting data up into test data and training data (test data is for year 2019, training is the rest)
```{r}
test_train_split <- function(dummy_bc, best.formula) {
  # data.test <- dummy_bc %>% filter(Year == 2019)
  # data.train <- dummy_bc %>% filter(Year != 2019)
  X <- model.matrix(best.formula, data = dummy_bc)[,-1]
  y <- as.matrix(dummy_bc %>% select(all.vars(best.formula)[1]))
  
  set.seed(0)
  train.i = sample(1:nrow(dummy_bc), 0.8*nrow(dummy_bc), replace = F)
  
  #train
  X.train <- X[train.i,]
  y.train <- y[train.i,]
  
  #test
  X.test <- X[-train.i,]
  y.test <- y[-train.i,]
  
  data.train <- as.data.frame(cbind(y.train, X.train))
  data.test <- as.data.frame(cbind(y.test, X.test))
  colnames(data.train)[1] = all.vars(best.formula)[1]
  colnames(data.test)[1] = all.vars(best.formula)[1]
  
  return (list('X.train' = X.train, 'y.train' = y.train, 'X.test' = X.test, 'y.test' = y.test, 'data.train' = data.train, 'data.test' = data.test))
}
```

```{r}
test_train_split_old <- function(dummy_bc, best.formula) {
  # data.test <- dummy_bc %>% filter(Year == 2019)
  # data.train <- dummy_bc %>% filter(Year != 2019)
  set.seed(0)
  train.i = sample(1:nrow(dummy_bc), 0.8*nrow(dummy_bc), replace = F)
  data.train <- dummy_bc %>% slice(train.i)
  data.test <- dummy_bc %>% slice(-train.i)
  
  #train
  #X.train <- as.matrix(data.train %>% select(-Avg.bc))
  X.train <- model.matrix(best.formula, data = data.train)[,-1]
  y.train <- as.matrix(data.train %>% select(all.vars(best.formula)[1]))

  #test
  #X.test <- as.matrix(data.test %>% select(-Avg.bc))
  X.test <- model.matrix(best.formula, data = data.test)[,-1]
  y.test <- as.matrix(data.test %>% select(all.vars(best.formula)[1]))
  
  #return (list('X.train' = X.train, 'y.train' = y.train, 'X.test' = X.test, 'y.test' = y.test, 'data.train' = data.train, 'data.test' = data.test))
}

#Income
income.data.split.sat <- test_train_split(income.dummy.bc, Avg.bc ~ .)
income.data.split <- test_train_split(income.dummy.bc, income.best.formula)

#Industry
industry.data.split.sat <- test_train_split(industry.dummy.bc, Avg.bc ~ .)
industry.data.split <- test_train_split(industry.dummy.bc, industry.best.formula)

test_train_split(income_cleaned, sat.formula)

```


Lasso regression for comparison to Forward Stepwise
```{r}
# calc_MSE <- function (model, x.test, y.test){
#   y.predict <- predict(model, newdata = x.test)
#   return(mean((y.predict - y.test)^2))
# }

# xy.splits <- list('income' = income.data.split, 
#                   'industry' = industry.data.split, 
#                   'income.sat' = income.data.split.sat, 
#                   'industry.sat' = industry.data.split.sat)
# 
# lasso.list <- c('income', 'industry', 'income.sat', 'industry.sat')
# baseline.list <- c('income.sat', 'industry.sat', 'income.sat', 'industry.sat')
# ridge.alpha <- 0
# 
# for (a in 1:4){
#   i = lasso.list[a]
#   b = baseline.list[a]
#   X.train <- xy.splits[[i]][['X.train']]
#   y.train <- xy.splits[[i]][['y.train']]
#   X.test <- xy.splits[[i]][['X.test']]
#   y.test <- xy.splits[[i]][['y.test']]
#   X.test.sat <- xy.splits[[b]][['X.test']]
#   y.test.sat <- xy.splits[[b]][['y.test']]
#   data.train <- xy.splits[[i]][['data.train']]
#   data.test <- xy.splits[[i]][['data.test']]
#   
#   #create lambda grid
#   lambda.grid = 10^seq(2, -5, length = 100)
#   
#   #create lasso models with lambda.grid
#   lasso.models = glmnet(X.train, y.train, alpha = ridge.alpha, lambda = lambda.grid)
#   
#   #visualize coefficient shrinkage
#   plot(lasso.models, xvar = "lambda", label = TRUE, main = paste("Lasso Regression:", i))
#   
#   #Cross Validation to find best lambda
#   set.seed(0)
#   cv.lasso.models <- cv.glmnet(X.train, y.train, alpha = ridge.alpha, lambda = lambda.grid, nfolds = 10)
#   
#   #visualize cross validation for lambda that minimizes the mean squared error.
#   plot(cv.lasso.models, main = paste("Lasso Regression:", i))
#   
#   #Checking the best lambda
#   log(cv.lasso.models$lambda.min)
#   best.lambda <- cv.lasso.models$lambda.min
#   print(paste(i, ' best.lambda:', best.lambda))
#   # best lambda with all the variables was found to be 0.0006892612
#   # best lambda with only the bwdBIC coefficients included was found to be 0.0003053856
#   
#   #looking at the lasso coefficients for the best.lambda
#   best.lambda.coeff <- predict(lasso.models, s = best.lambda, type = "coefficients")
#   print('Number of Coefficients:')
#   print(dim(best.lambda.coeff)[1])
#   
#   #fitting a model with the best lambda found to be 0.000689 and using it to make predictions for the testing data.
#   lasso.best.lambda.train.pred <- predict(lasso.models, s = best.lambda, newx = X.test)
#   lasso.best.lambda.train.pred
#   
#   #checking MSE
#   MSE.lasso <- mean((lasso.best.lambda.train.pred - y.test)^2)
#   sat.model.bc <- lm(Avg.bc ~., data = data.train)
#   
#   
#   temp.df <- as.data.frame(X.test.sat) #temp fix
#   colnames(temp.df) <- str_replace_all(colnames(temp.df), "[`]", '')
#   
#   y.predict <- predict(sat.model.bc, newdata = temp.df)
#   MSE.sat <- mean((y.predict - y.test.sat)^2)
#   
#   print(paste(i, ' Lasso MSE: ', MSE.lasso, ' ', b, ' Saturated MSE: ', MSE.sat))
# 
#   metrics <- eval_results(y.test, lasso.best.lambda.train.pred, data.test)
#   print(metrics)
#   print('********************************')
# }
```

Function to show metrics (R^2 and MSE) for Regularization (Ridge/Lasso)
```{r}
regularization_func <- function (data, alpha, name){
  X.train <- data[['X.train']]
  y.train <- data[['y.train']]
  X.test <- data [['X.test']]
  y.test <- data[['y.test']]
  data.test <- data[['data.test']]
  # print(colnames(X.train))
  # print(colnames(X.test))
  # print(setdiff(colnames(X.train), colnames(X.test)))
  
  #create lambda grid
  lambda.grid = 10^seq(10, -10, length = 100)
  
  #create lasso models with lambda.grid
  lasso.models = glmnet(X.train, y.train, alpha = ridge.alpha, lambda = lambda.grid)
  
  #visualize coefficient shrinkage
  # plot(lasso.models, xvar = "lambda", label = TRUE, main = paste("Lasso Regression:", i))
  
  #Cross Validation to find best lambda
  set.seed(0)
  cv.lasso.models <- cv.glmnet(X.train, y.train, alpha = ridge.alpha, lambda = lambda.grid, nfolds = 10)
  
  #visualize cross validation for lambda that minimizes the mean squared error.
  plot(cv.lasso.models, main = paste("Alpha:", alpha, "Regression:", name))
  
  #Checking the best lambda
  # log(cv.lasso.models$lambda.min)
  # best.lambda <- cv.lasso.models$lambda.min
  # print(paste(i, ' best.lambda:', best.lambda))
  # best lambda with all the variables was found to be 0.0006892612
  # best lambda with only the bwdBIC coefficients included was found to be 0.0003053856
  
  #looking at the lasso coefficients for the best.lambda
  # best.lambda.coeff <- predict(lasso.models, s = cv.lasso.models$lambda.min, type = "coefficients")
  # print('Number of Coefficients:')
  # print(dim(best.lambda.coeff)[1])
  
  #fitting a model with the best lambda found to be 0.000689 and using it to make predictions for the testing data.
  lasso.best.lambda.train.pred <- predict(lasso.models, s = cv.lasso.models$lambda.min, newx = X.test)
  lasso.best.lambda.train.pred
  
  #checking MSE
  MSE.lasso <- mean((lasso.best.lambda.train.pred - y.test)^2)
  print(paste('Dimensions of the Alpha:', alpha, ' Regression Coefficients for:', name))
  print(dim(coef(lasso.models))[1])
  p = dim(coef(lasso.models))[1]
  return(eval_results(y.test, lasso.best.lambda.train.pred, data.test, p)['Rsquare'])
}

regularization_func(all.splits[['income_cleaned']], 0, 'income_cleaned')
regularization_func(all.splits[['income_cleaned_bc']], 0, 'income_cleaned_bc')
regularization_func(all.splits[['income.data.split.sat']], 0, 'income.data.split.sat')
```


```{r}
#all.dfs <- list(income_cleaned, income_cleaned_bc, income.dummy.bc)

formulas <- list(income_cleaned = sat.formula, 
                 income_cleaned_bc = sat.formula.bc, 
                 income.data.split.sat = sat.formula.bc, 
                 income.data.split.best = income.best.formula,
                 industry_cleaned = sat.formula, 
                 industry_cleaned_bc = sat.formula.bc, 
                 industry.data.split.sat = sat.formula.bc, 
                 industry.data.split.best = industry.best.formula)

all.splits <- list(
  'income_cleaned' = test_train_split(income_cleaned, formulas[['income_cleaned']]),
  'income_cleaned_bc' = test_train_split(income_cleaned_bc, formulas[['income_cleaned_bc']]),
  'income.data.split.sat' = test_train_split(income.dummy.bc, formulas[['income.data.split.sat']]),
  'income.data.split.best' = test_train_split(income.dummy.bc, formulas[['income.data.split.best']]),
  'industry_cleaned' = test_train_split(industry_cleaned, formulas[['industry_cleaned']]),
  'industry_cleaned_bc' = test_train_split(industry_cleaned_bc, formulas[['industry_cleaned_bc']]),
  'industry.data.split.sat' = test_train_split(industry.dummy.bc, formulas[['industry.data.split.sat']]),
  'industry.data.split.best' = test_train_split(industry.dummy.bc, formulas[['industry.data.split.best']])
)

#For loops initialization
no.reg.r2 <- c() 
lasso.reg.r2 <- c()
ridge.reg.r2 <- c()


for (i in names(all.splits)) {
  #no regularization
  m = lm(formulas[[i]], all.splits[[i]][['data.train']])
  #no.reg.r2 <- c(no.reg.r2, summary(m)$adj.r.squared)
  y.predict = predict(m, newdata = as.data.frame(all.splits[[i]][['X.test']]))
  adj.R2 <- eval_results(all.splits[[i]][['y.test']], y.predict, all.splits[[i]][['data.test']], length(coef(m)))['Rsquare']
  no.reg.r2 <- c(no.reg.r2, adj.R2)
  
  #lasso regularization
  lasso.reg.r2 <- c(lasso.reg.r2, regularization_func(all.splits[[i]], 1, i))
  
  #ridge regularization
  ridge.reg.r2 <- c(ridge.reg.r2, regularization_func(all.splits[[i]], 0, i))
}

```


```{r}
df.rsquare <- as.data.frame(cbind('noReg' = no.reg.r2, 'lassoReg' = lasso.reg.r2, 'ridgeReg' = ridge.reg.r2))
rownames(df.rsquare) = names(all.splits)
df.rsquare
```


```{r}
# sat.model.bc <- lm(Avg.bc ~., data = xy.splits.sat[['industry']][['data.train']])
# 
# temp.df <- as.data.frame(xy.splits.sat[['industry']][['X.test']])
# 
# colnames(temp.df)[57]
# 
# colnames(temp.df) <- str_replace_all(colnames(temp.df), "[`]", '')
# colnames(temp.df)
# 
# y.predict <- predict(sat.model.bc, newdata = temp.df)
# MSE.sat <- mean((y.predict - xy.splits.sat[['industry']][['y.test']])^2)
# 
# print(paste(i, ' Lasso MSE: ', MSE.lasso, ' Saturated MSE: ', MSE.sat))
# 
# summary(sat.model.bc)$coefficients
# as.data.frame(xy.splits.sat[['industry']][['X.test']]) %>% select(starts_with('`GroupOth'))

```


```{r}
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df, p) {
  n = nrow(df)
  adj.RSS <- sum((predicted - true)^2)/(n-p-1)
  adj.TSS <- sum((true - mean(true))^2)/(n-1)
  adj.R_square <- 1 - adj.RSS / adj.TSS
  #RMSE = sqrt(RSS/n)
  
  return(c(Rsquare = adj.R_square))
}

#formula for adjusted R^2 found in following link.
#https://www.graphpad.com/guides/prism/latest/curve-fitting/reg_adjusted-r-squared.htm
```







```{r}
as.data.frame(lasso.best.lambda.train.pred) %>% mutate(Avg_in_dollars = (lasso.best.lambda.train.pred*lambda.bc+1)^(1/lambda.bc))
```

```{r}
income.sat.data.split <- test_train_split(income.dummy.bc, Avg.bc ~ .)
income.sat.data.split[['X.train']]

sat.model.bc <- lm(Avg.bc ~., data = as.data.frame(cbind(income.sat.data.split[['X.train']], income.sat.data.split[['y.train']])))
summary(sat.model.bc)

calc_MSE(sat.model.bc, as.data.frame(income.sat.data.split[['X.test']]), income.sat.data.split[['y.test']])


sat.y.predict <- predict(sat.model.bc, newdata = as.data.frame(income.sat.data.split[['X.test']]))
mean((sat.y.predict - income.sat.data.split[['y.test']])^2)
```

