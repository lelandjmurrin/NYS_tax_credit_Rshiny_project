---
title: "R Notebook"
output: html_notebook
---
Call Libraries
```{r} 
library(tidyverse)
library(caret)
library(MASS)
library(car)
library(moments)
```


Calling the Transformed Datasets
```{r}
income_cleaned = read_csv('NYS_Corp_Tax_Credit_data/income_cleaned.csv')
income_cleaned
industry_cleaned = read_csv('NYS_Corp_Tax_Credit_data/industry_cleaned.csv')
industry_cleaned
```


Creating the Models
```{r}
sat.model.summary <- function (df, field){
    
    f = as.formula(paste(field,' ~ . - Amount'))
    
    #Shapiro-Wilks test to evaluate normality
    print(shapiro.test(df[[field]]))
    
    #Kurtosis evaluation (normal distribution has a value close to 3)
    print(kurtosis(df[[field]]))
    linear.model.cleaned <- lm(f, data = df)
    print(summary(linear.model.cleaned))
    plot(linear.model.cleaned)
    
    #histograms of response variable to check distribution
    print(df %>% 
      ggplot(aes_string(field)) + 
      geom_histogram() + 
      labs(title = 'Average Credit Amount Distribution') + 
      theme(plot.title = element_text(hjust = 0.5))) 

    return (linear.model.cleaned)
}

income.lm.cleaned <- sat.model.summary(income_cleaned, 'Avg')
industry.lm.cleaned <- sat.model.summary(industry_cleaned, 'Avg')
```

```{r}
# linear.model.cleaned2 <- lm(Avg ~ . - Amount, data = income_cleaned)
# s = summary(linear.model.cleaned)
# show(s)
# plot(linear.model.cleaned)
# 
# #histograms of response variable to check distribution
# hist(income_cleaned$Avg)
# hist(income_cleaned_bc$Avg.bc)
# 
# #Shapiro-Wilks test to evaluate normality
# shapiro.test(income_cleaned$Avg)
# shapiro.test(income_cleaned_bc$Avg.bc)
# 
# #Kurtosis evaluation (normal distribution has a value close to 3)
# moments::kurtosis(income_cleaned$Avg)
# moments::kurtosis(income_cleaned_bc$Avg.bc) #evaluated to 2.75 which means very close to 3 so it is normally distributed with possibly slightly less outliers. 
```

Correcting violation of Normality in previous model with BoxCox transform
```{r}
bc = boxCox(income.lm.cleaned, lambda = seq(-0.2, 0.2, 1/10))
lambda.bc = bc$x[which(bc$y == max(bc$y))] #Extracting the best lambda value.
lambda.bc #best lambda was found to be -0.020202 with confidence interval that doesn't include 0 or 1 therefore the (y^lambda-1)/lambda transform is appropriate

income_cleaned_bc <- income_cleaned %>% mutate(Avg.bc = (Avg^lambda.bc -1)/lambda.bc) %>% select(-c(Avg, Amount))
income_cleaned_bc
model.bc = lm(Avg.bc ~ ., data = income_cleaned_bc)
summary(model.bc)
```

Visualization and model stats
```{r}
plot(model.bc)
vif(model.bc)
influencePlot(model.bc)
avPlots(model.bc)
BIC(model.bc, linear.model.cleaned)
```

Stepwise Regression on Income_cat_bc (boxcox transformed dataset)
```{r}
#creating dummy variable columns for stepwise
x = model.matrix(Avg.bc ~., income_cleaned_bc)[, -1]
dummy_bc = as.data.frame(x) %>% mutate(Avg.bc = income_cleaned_bc$Avg.bc)

#cleaning column names so stepwise regression doesn't present any errors
colnames(dummy_bc) <- str_replace_all(colnames(dummy_bc), "-|'|/| |,|ï¿½" , '_')
colnames(dummy_bc)[37] <- 'NameManufactureru0092s_Real_Property_Tax_Credit'
colnames(dummy_bc)

#creating models
model.empty = lm(Avg.bc ~ 1, data = dummy_bc) #intercept only
model.full = lm(Avg.bc ~ ., data = dummy_bc) #All variables
scope = list(lower = formula(model.empty), upper = formula(model.full))
n_obs = dummy_bc %>% count() %>% first()
```

Stepwise regression using BIC as the criteria (the penalty k = log(n)).
```{r}
#Forward BIC stepwise
forwardBIC = step(model.empty, scope, direction = "forward", k = log(n_obs))
fwd.BIC.summary.ENI <- data.frame(summary(forwardBIC)$coefficients)
fwd.BIC.summary.ENI %>% arrange(rownames(fwd.BIC.summary.ENI))

#Backward BIC stepwise
backwardBIC = step(model.full, scope, direction = "backward", k = log(n_obs))
bwd.BIC.summary.ENI <- data.frame(summary(backwardBIC)$coefficients)
bwd.BIC.summary.ENI %>% arrange(rownames(bwd.BIC.summary.ENI))

summary(forwardBIC)
summary(backwardBIC)

#inputing forwardBIC into a dataframe with index created as a column
fwd.BIC.summary.ENI$ENI = rownames(fwd.BIC.summary.ENI)
rownames(fwd.BIC.summary.ENI) = NULL
fwd.BIC.summary.ENI

#inputing backwardBIC into a dataframe with index created as a column
bwd.BIC.summary.ENI$ENI = rownames(bwd.BIC.summary.ENI)
rownames(bwd.BIC.summary.ENI) = NULL
bwd.BIC.summary.ENI

#comparing the forward and backward stepwise regression coefficients
anti_join(fwd.BIC.summary.ENI, bwd.BIC.summary.ENI, by = 'ENI')
inner_join(fwd.BIC.summary.ENI, bwd.BIC.summary.ENI, by = 'ENI')

#choosing backwardBIC because of negligible reduction in adjusted R^2 but we can reduce our sizable number of predictor variables.

#checking our selected model's predictor variable's VIFs
vif(backwardBIC)

best.formula <- backwardBIC$call[[2]]
```


Splitting data up into test data and training data (test data is for year 2019, training is the rest)
```{r}
data.test <- dummy_bc %>% filter(Year == 2019)
data.train <- dummy_bc %>% filter(Year != 2019)

#training model based on best.formula from stepwise
train.model <- lm(best.formula, data = data.train)
summary(train.model)

#train
#X.train <- as.matrix(data.train %>% select(-Avg.bc))
X.train <- model.matrix(best.formula, data = data.train)[,-1]
y.train <- as.matrix(data.train %>% select(Avg.bc))

#test
#X.test <- as.matrix(data.test %>% select(-Avg.bc))
X.test <- model.matrix(best.formula, data = data.test)[,-1]
y.test <- as.matrix(data.test %>% select(Avg.bc))

dim(X.train)
dim(X.test)
```


Lasso regression for comparison to backward stepwise
```{r}
#create lambda grid
lambda.grid = 10^seq(2, -5, length = 100)

#create lasso models with lambda.grid
lasso.models = glmnet(X.train, y.train, alpha = 1, lambda = lambda.grid)

#visualize coefficient shrinkage
plot(lasso.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")

#Cross Validation to find best lambda
set.seed(0)
cv.lasso.models <- cv.glmnet(X.train, y.train, alpha = 1, lambda = lambda.grid, nfolds = 10)

#visualize cross validation for lambda that minimizes the mean squared error.
plot(cv.lasso.models, main = "Lasso Regression")

#Checking the best lambda
log(cv.lasso.models$lambda.min)
best.lambda <- cv.lasso.models$lambda.min
best.lambda
# best lambda with all the variables was found to be 0.0006892612
# best lambda with only the bwdBIC coefficients included was found to be 0.0003053856

#looking at the lasso coefficients for the best.lambda
best.lambda.coeff <- predict(lasso.models, s = best.lambda, type = "coefficients")
best.lambda.coeff

#fitting a model with the best lambda found to be 0.000689 and using it to make predictions for the testing data.
lasso.best.lambda.train.pred <- predict(lasso.models, s = best.lambda, newx = X.test)
lasso.best.lambda.train.pred

#checking MSE
mean((lasso.best.lambda.train.pred - y.test)^2)
```

```{r}
as.data.frame(lasso.best.lambda.train.pred) %>% mutate(Avg_in_dollars = (lasso.best.lambda.train.pred*lambda.bc+1)^(1/lambda.bc))
```

