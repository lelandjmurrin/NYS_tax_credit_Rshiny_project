---
title: "R Notebook"
output: html_notebook
---

```{r} 
#Call Libraries
library(tidyverse)
library(caret)
library(MASS)
library(car)
```


Calling the Transformed Datasets
```{r}
income_cleaned = read_csv('NYS_Corp_Tax_Credit_data/income_cleaned.csv')
income_cleaned
```


Creating the Models
```{r}
linear.model.cleaned <- lm(Avg ~ . -Amount, data = income_cleaned)
summary(linear.model.cleaned)
plot(linear.model.cleaned)
```

```{r}
bc = boxCox(linear.model.cleaned)
lambda.bc = bc$x[which(bc$y == max(bc$y))] #Extracting the best lambda value.
lambda.bc #best lambda was found to be -0.020202

income_cleaned_bc <- income_cleaned %>% mutate(Avg.bc = (Avg^lambda.bc -1)/lambda.bc) %>% select(-c(Avg, Amount))
income_cleaned_bc
model.bc = lm(Avg.bc ~ ., data = income_cleaned_bc)
summary(model.bc)
```

Visualization and model stats
```{r}
plot(model.bc)
vif(model.bc)
influencePlot(model.bc)
avPlots(model.bc)
BIC(model.bc, linear.model.cleaned)
```

Stepwise Regression on Income_cat_bc (boxcox transformed dataset)
```{r}
#creating dummy variable columns for stepwise
x = model.matrix(Avg.bc ~., income_cleaned_bc)[, -1]
dummy_bc = as.data.frame(x) %>% mutate(Avg.bc = income_cleaned_bc$Avg.bc)

#cleaning column names so stepwise regression doesn't present any errors
colnames(dummy_bc) <- str_replace_all(colnames(dummy_bc), "-|'|/| |,|ï¿½" , '_')
colnames(dummy_bc)[37] <- 'NameManufactureru0092s_Real_Property_Tax_Credit'
colnames(dummy_bc)

#creating models
model.empty = lm(Avg.bc ~ 1, data = dummy_bc) #intercept only
model.full = lm(Avg.bc ~ ., data = dummy_bc) #All variables
scope = list(lower = formula(model.empty), upper = formula(model.full))
n_obs = dummy_bc %>% count() %>% first()
```

Stepwise regression using BIC as the criteria (the penalty k = log(n)).
```{r}
#Forward BIC stepwise
forwardBIC = step(model.empty, scope, direction = "forward", k = log(n_obs))
fwd.BIC.summary.ENI <- data.frame(summary(forwardBIC)$coefficients)
fwd.BIC.summary.ENI %>% arrange(rownames(fwd.BIC.summary.ENI))

#Backward BIC stepwise
backwardBIC = step(model.full, scope, direction = "backward", k = log(n_obs))
bwd.BIC.summary.ENI <- data.frame(summary(backwardBIC)$coefficients)
bwd.BIC.summary.ENI %>% arrange(rownames(bwd.BIC.summary.ENI))

summary(forwardBIC)
summary(backwardBIC)

#inputing forwardBIC into a dataframe with index created as a column
fwd.BIC.summary.ENI$ENI = rownames(fwd.BIC.summary.ENI)
rownames(fwd.BIC.summary.ENI) = NULL
fwd.BIC.summary.ENI

#inputing backwardBIC into a dataframe with index created as a column
bwd.BIC.summary.ENI$ENI = rownames(bwd.BIC.summary.ENI)
rownames(bwd.BIC.summary.ENI) = NULL
bwd.BIC.summary.ENI

#comparing the forward and backward stepwise regression coefficients
anti_join(fwd.BIC.summary.ENI, bwd.BIC.summary.ENI, by = 'ENI')
inner_join(fwd.BIC.summary.ENI, bwd.BIC.summary.ENI, by = 'ENI')

#choosing backwardBIC because of negligible reduction in adjusted R^2 but we can reduce our sizable number of predictor variables.

#checking our selected model's predictor variable's VIFs
vif(backwardBIC)

best.formula <- backwardBIC$call[[2]]
```


Splitting data up into test data and training data (test data is for year 2019, training is the rest)
```{r}
data.test <- dummy_bc %>% filter(Year == 2019)
data.train <- dummy_bc %>% filter(Year != 2019)

#training model based on best.formula from stepwise
train.model <- lm(best.formula, data = data.train)
summary(train.model)

#train
#X.train <- as.matrix(data.train %>% select(-Avg.bc))
X.train <- model.matrix(best.formula, data = data.train)[,-1]
y.train <- as.matrix(data.train %>% select(Avg.bc))

#test
#X.test <- as.matrix(data.test %>% select(-Avg.bc))
X.test <- model.matrix(best.formula, data = data.test)[,-1]
y.test <- as.matrix(data.test %>% select(Avg.bc))
dim(X.train)
dim(X.test)
```


Lasso regression for comparison to backward stepwise
```{r}
#create lambda grid
lambda.grid = 10^seq(2, -5, length = 100)

#create lasso models with lambda.grid
lasso.models = glmnet(X.train, y.train, alpha = 1, lambda = lambda.grid)

#visualize coefficient shrinkage
plot(lasso.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")

#Cross Validation to find best lambda
set.seed(0)
cv.lasso.models <- cv.glmnet(X.train, y.train, alpha = 1, lambda = lambda.grid, nfolds = 10)

#visualize cross validation for lambda that minimizes the mean squared error.
plot(cv.lasso.models, main = "Lasso Regression")

#Checking the best lambda
log(cv.lasso.models$lambda.min)
best.lambda <- cv.lasso.models$lambda.min
best.lambda
# best lambda with all the variables was found to be 0.0006892612
# best lambda with only the bwdBIC coefficients included was found to be 0.0003053856

#looking at the lasso coefficients for the best.lambda
best.lambda.coeff <- predict(lasso.models, s = best.lambda, type = "coefficients")
best.lambda.coeff

#fitting a model with the best lambda found to be 0.000689 and using it to make predictions for the testing data.
lasso.best.lambda.train.pred <- predict(lasso.models, s = best.lambda, newx = X.test)
lasso.best.lambda.train.pred

#checking MSE
mean((lasso.best.lambda.train.pred - y.test)^2)

```