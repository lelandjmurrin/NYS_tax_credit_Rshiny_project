---
title: "R Notebook"
output: html_notebook
---
Call Libraries
```{r} 
library(tidyverse)
library(caret)
library(MASS)
library(car)
library(moments)
```


Calling the Transformed Datasets
```{r}
income_cleaned = read_csv('NYS_Corp_Tax_Credit_data/income_cleaned.csv')
income_cleaned
industry_cleaned = read_csv('NYS_Corp_Tax_Credit_data/industry_cleaned.csv')
industry_cleaned
```


Creating the Models
```{r}
sat.model.summary <- function (df, field, sat.formula){
    
    #Shapiro-Wilks test to evaluate normality
    print(shapiro.test(df[[field]]))
    
    #Kurtosis evaluation (normal distribution has a value close to 3)
    print('kurtosis')
    print(kurtosis(df[[field]]))
    linear.model.cleaned = lm(sat.formula, data = df)
    print(summary(linear.model.cleaned))
    plot(linear.model.cleaned)
    
    #histograms of response variable to check distribution
    print(df %>% 
      ggplot(aes_string(field)) + 
      geom_histogram() + 
      labs(title = 'Average Credit Amount Distribution') + 
      theme(plot.title = element_text(hjust = 0.5)))
    
    #Checking multicollinearity using VIF measurement
    print(vif(linear.model.cleaned))
    influencePlot(linear.model.cleaned)
    #avPlots(linear.model.cleaned)
}


sat.formula <- Avg ~ . - Amount
sat.field <- 'Avg'

sat.model.summary(income_cleaned, sat.field, sat.formula)
income.model <- lm(sat.formula, data = income_cleaned)

sat.model.summary(industry_cleaned, sat.field, sat.formula)
industry.model <- lm(sat.formula, data = industry_cleaned)
```

Correcting violation of Normality in previous model with BoxCox transform
```{r}
bc_func <- function (lm.cleaned, lambda.range){
  bc = boxCox(lm.cleaned, lambda = lambda.range)
  #Extracting the best lambda value.
  return(bc$x[which(bc$y == max(bc$y))])
}
  
#Income Group Dataset
income.lambda.bc = bc_func(income.model, seq(-0.2, 0.2, 1/10))
income.lambda.bc

#Industry Group Dataset
industry.lambda.bc = bc_func(industry.model, seq(-0.2, 0.2, 1/10))
industry.lambda.bc

bc_transform <- function(df, lambda.bc){
  return (df %>% 
            mutate(Avg.bc = (Avg^lambda.bc -1)/lambda.bc) %>% 
            select(-c(Avg, Amount)))
}

#Income Group Dataset
income_cleaned_bc <- bc_transform(income_cleaned, income.lambda.bc)
income.model.bc = lm(Avg.bc ~ ., data = income_cleaned_bc)

#Industry Group Dataset
industry_cleaned_bc <- bc_transform(industry_cleaned, industry.lambda.bc)
industry.model.bc = lm(Avg.bc ~ ., data = industry_cleaned_bc)
```

Checking linear regression assumptions for the transformed data.
```{r}
sat.formula.bc <- Avg.bc ~ .
sat.field.bc <- 'Avg.bc'

#Income
sat.model.summary(income_cleaned_bc, sat.field.bc, sat.formula.bc)

#Industry
sat.model.summary(industry_cleaned_bc, sat.field.bc, sat.formula.bc)
```

BIC comparison before and after BoxCox transform
```{r}
BIC(income.model.bc, income.model)
BIC(industry.model.bc, industry.model)
```

Stepwise Regression on Income_cat_bc (boxcox transformed dataset)
```{r}
#creating dummy variable columns for stepwise
dummy_func <- function (df){
  x = model.matrix(Avg.bc ~., df)[, -1]
  dummy_bc = as.data.frame(x) %>% mutate(Avg.bc = df$Avg.bc)
  colnames(dummy_bc) <- str_replace_all(colnames(dummy_bc), "-|'|/| |,|ï¿½|&" , '_')
  return(dummy_bc)
}
```

Cleaning column names further so stepwise regression doesn't present any errors
```{r}
#Income Group Dataset
income.dummy.bc <- dummy_func(income_cleaned_bc)
#colnames(income.dummy.bc)[37] <- 'NameManufactureru0092s_Real_Property_Tax_Credit'
colnames(income.dummy.bc)

#Industry Group Dataset
industry.dummy.bc <- dummy_func(industry_cleaned_bc)
#colnames(industry.dummy.bc)[35] <- 'NameManufactureru0092s_Real_Property_Tax_Credit'
colnames(industry.dummy.bc)
```

Stepwise regression using BIC as the criteria (the penalty k = log(n)).
```{r}
#Creating Stepwise Models
bcs = list(income = income.dummy.bc, industry = industry.dummy.bc)
model.fulls = list(income = lm(Avg.bc ~ ., data = income.dummy.bc), industry = lm(Avg.bc ~ ., data = industry.dummy.bc)) #All variables
model.emptys = list(income = lm(Avg.bc ~ 1, data = income.dummy.bc), industry = lm(Avg.bc ~ 1, data = industry.dummy.bc)) #intercept only
k = c('income', 'industry')
forwardBIC = list(income = NULL, industry = NULL)
backwardBIC = list(income = NULL, industry = NULL)

for (i in k){
  bc = bcs[[i]]
  scope = list(lower = formula(model.emptys[[i]]), upper = formula(model.fulls[[i]]))
  n_obs = bc %>% count() %>% dplyr::first()
  forwardBIC[[i]] = step(model.emptys[[i]], scope, direction = "forward", k = log(n_obs))
  backwardBIC[[i]] = step(model.fulls[[i]], scope, direction = "backward", k = log(n_obs))
}
```

Selecting Best Formula per Dataset from Stepwise Regressions
```{r}
bic_func <- function (BIC.model, sat.model){
  print('Adjusted R Squared:')
  print(summary(BIC.model)$adj.r.squared)
  print('Number of Coefficients:')
  print(dim(summary(BIC.model)$coefficient)[1])
  print('VIF Check: ')
  print(max(vif(BIC.model)))
  print('Anova test p-value (between saturated model and stepwise):')
  print(anova(BIC.model, sat.model)$`Pr(>F)`[2])
  print("*************************")
}

a = anova(forwardBIC[['income']], model.fulls[['income']])

a$`Pr(>F)`[2]

bic_func(forwardBIC[['income']], model.fulls[['income']])
bic_func(backwardBIC[['income']], model.fulls[['income']])
bic_func(forwardBIC[['industry']], model.fulls[['industry']])
bic_func(backwardBIC[['industry']], model.fulls[['industry']])

bic_func(model.fulls[['industry']], model.fulls[['industry']])


VIF.variables <- as.data.frame(vif(model.fulls[['industry']])) %>% 
  select(VIF = `vif(model.fulls[["industry"]])`) %>% 
  filter(VIF > 5) %>% rownames()

industry.dummy.bc %>% select(-all_of(VIF.variables))

```


```{r}
#anova(model2, model.saturated)

#Income Group Dataset
income.best.formula <- backwardBIC[['income']]$call[[2]]
income.best.formula

#Industry Group Dataset
industry.best.formula <- forwardBIC[['industry']]$call[[2]]
industry.best.formula
```

Stepwise regression using BIC as the criteria (the penalty k = log(n)).
```{r}
# #Forward BIC stepwise
# forwardBIC = step(model.empty, scope, direction = "forward", k = log(n_obs))
# fwd.BIC.summary.ENI <- data.frame(summary(forwardBIC)$coefficients)
# fwd.BIC.summary.ENI %>% arrange(rownames(fwd.BIC.summary.ENI))
# 
# #Backward BIC stepwise
# backwardBIC = step(model.full, scope, direction = "backward", k = log(n_obs))
# bwd.BIC.summary.ENI <- data.frame(summary(backwardBIC)$coefficients)
# bwd.BIC.summary.ENI %>% arrange(rownames(bwd.BIC.summary.ENI))
# 
# summary(forwardBIC)
# summary(backwardBIC)
# 
# #inputing forwardBIC into a dataframe with index created as a column
# fwd.BIC.summary.ENI$ENI = rownames(fwd.BIC.summary.ENI)
# rownames(fwd.BIC.summary.ENI) = NULL
# fwd.BIC.summary.ENI
# 
# #inputing backwardBIC into a dataframe with index created as a column
# bwd.BIC.summary.ENI$ENI = rownames(bwd.BIC.summary.ENI)
# rownames(bwd.BIC.summary.ENI) = NULL
# bwd.BIC.summary.ENI
# 
# #comparing the forward and backward stepwise regression coefficients
# anti_join(fwd.BIC.summary.ENI, bwd.BIC.summary.ENI, by = 'ENI')
# inner_join(fwd.BIC.summary.ENI, bwd.BIC.summary.ENI, by = 'ENI')
# 
# #choosing backwardBIC because of negligible reduction in adjusted R^2 but we can reduce our sizable number of predictor variables.
# 
# #checking our selected model's predictor variable's VIFs
# vif(backwardBIC)
# 
# best.formula <- backwardBIC$call[[2]]
```


Splitting data up into test data and training data (test data is for year 2019, training is the rest)
```{r}
test_train_split <- function(dummy_bc, best.formula) {
  data.test <- dummy_bc %>% filter(Year == 2019)
  data.train <- dummy_bc %>% filter(Year != 2019)
  
  #train
  #X.train <- as.matrix(data.train %>% select(-Avg.bc))
  X.train <- model.matrix(best.formula, data = data.train)[,-1]
  y.train <- as.matrix(data.train %>% select(Avg.bc))

  #test
  #X.test <- as.matrix(data.test %>% select(-Avg.bc))
  X.test <- model.matrix(best.formula, data = data.test)[,-1]
  y.test <- as.matrix(data.test %>% select(Avg.bc))
  return (list('X.train' = X.train, 'y.train' = y.train, 'X.test' = X.test, 'y.test' = y.test))
}


income.data.split <- test_train_split(income.dummy.bc, income.best.formula)
income.data.split[['y.train']]

industry.data.split <- test_train_split(industry.dummy.bc, industry.best.formula)

# data.test <- dummy_bc %>% filter(Year == 2019)
# data.train <- dummy_bc %>% filter(Year != 2019)
# 
# #training model based on best.formula from stepwise
# train.model <- lm(best.formula, data = data.train)
# summary(train.model)
# 
# #train
# #X.train <- as.matrix(data.train %>% select(-Avg.bc))
# X.train <- model.matrix(best.formula, data = data.train)[,-1]
# y.train <- as.matrix(data.train %>% select(Avg.bc))
# 
# #test
# #X.test <- as.matrix(data.test %>% select(-Avg.bc))
# X.test <- model.matrix(best.formula, data = data.test)[,-1]
# y.test <- as.matrix(data.test %>% select(Avg.bc))
# 
# dim(X.train)
# dim(X.test)
```


Lasso regression for comparison to backward stepwise
```{r}
calc_MSE <- function (model, x.test, y.test){
  y.predict <- predict(model, newdata = x.test)
  return(mean((y.predict - y.test)^2))
}

xy.sat.splits <- list('income' = income.dummy.bc, 'industry' = industry.dummy.bc)
xy.splits <- list('income' = income.data.split, 'industry' = industry.data.split)

for (i in k){
  X.train <- xy.splits[[i]][['X.train']]
  y.train <- xy.splits[[i]][['y.train']]
  X.test <- xy.splits[[i]][['X.test']]
  y.test <- xy.splits[[i]][['y.test']]
  
  #create lambda grid
  lambda.grid = 10^seq(2, -5, length = 100)
  
  #create lasso models with lambda.grid
  lasso.models = glmnet(X.train, y.train, alpha = 1, lambda = lambda.grid)
  
  #visualize coefficient shrinkage
  plot(lasso.models, xvar = "lambda", label = TRUE, main = paste("Lasso Regression:", i))
  
  #Cross Validation to find best lambda
  set.seed(0)
  cv.lasso.models <- cv.glmnet(X.train, y.train, alpha = 1, lambda = lambda.grid, nfolds = 10)
  
  #visualize cross validation for lambda that minimizes the mean squared error.
  plot(cv.lasso.models, main = paste("Lasso Regression:", i))
  
  #Checking the best lambda
  log(cv.lasso.models$lambda.min)
  best.lambda <- cv.lasso.models$lambda.min
  print(paste(i, ' best.lambda:', best.lambda))
  # best lambda with all the variables was found to be 0.0006892612
  # best lambda with only the bwdBIC coefficients included was found to be 0.0003053856
  
  #looking at the lasso coefficients for the best.lambda
  best.lambda.coeff <- predict(lasso.models, s = best.lambda, type = "coefficients")
  print(best.lambda.coeff)
  
  #fitting a model with the best lambda found to be 0.000689 and using it to make predictions for the testing data.
  lasso.best.lambda.train.pred <- predict(lasso.models, s = best.lambda, newx = X.test)
  lasso.best.lambda.train.pred
  
  #checking MSE
  MSE.lasso <- mean((lasso.best.lambda.train.pred - y.test)^2)
  sat.data.split <- test_train_split(xy.sat.splits[[i]], Avg.bc ~ .)
  sat.model.bc <- lm(Avg.bc ~., data = as.data.frame(cbind(sat.data.split[['X.train']], sat.data.split[['y.train']])))
  MSE.sat <- calc_MSE(sat.model.bc, as.data.frame(sat.data.split[['X.test']]), sat.data.split[['y.test']])
  
  print(paste(i, ' Lasso MSE: ', MSE.lasso, ' Saturated MSE: ', MSE.sat))
}

```

```{r}
as.data.frame(lasso.best.lambda.train.pred) %>% mutate(Avg_in_dollars = (lasso.best.lambda.train.pred*lambda.bc+1)^(1/lambda.bc))
```

```{r}
income.sat.data.split <- test_train_split(income.dummy.bc, Avg.bc ~ .)
income.sat.data.split[['X.train']]

sat.model.bc <- lm(Avg.bc ~., data = as.data.frame(cbind(income.sat.data.split[['X.train']], income.sat.data.split[['y.train']])))
summary(sat.model.bc)

calc_MSE(sat.model.bc, as.data.frame(income.sat.data.split[['X.test']]), income.sat.data.split[['y.test']])


sat.y.predict <- predict(sat.model.bc, newdata = as.data.frame(income.sat.data.split[['X.test']]))
mean((sat.y.predict - income.sat.data.split[['y.test']])^2)
```

